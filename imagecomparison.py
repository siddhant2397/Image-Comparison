# -*- coding: utf-8 -*-
"""Untitled17.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18i27efVAzIaGFzOqDk-hJdG0oLQrSsUd
"""

import streamlit as st
import cv2
import numpy as np
from PIL import Image

st.title("SURF Image Feature Comparison")

# Upload images
img1 = st.file_uploader("Upload First Image", type=['jpg', 'png', 'jpeg'])
img2 = st.file_uploader("Upload Second Image", type=['jpg', 'png', 'jpeg'])

if img1 and img2:
    # Convert images to OpenCV format
    image1 = np.array(Image.open(img1).convert('RGB'))
    image2 = np.array(Image.open(img2).convert('RGB'))
    image1_gray = cv2.cvtColor(image1, cv2.COLOR_RGB2GRAY)
    image2_gray = cv2.cvtColor(image2, cv2.COLOR_RGB2GRAY)

    # Initialize SURF
    surf = cv2.xfeatures2d.SURF_create(400)

    # Detect keypoints and descriptors
    kp1, des1 = surf.detectAndCompute(image1_gray, None)
    kp2, des2 = surf.detectAndCompute(image2_gray, None)

    # Match using FLANN matcher
    index_params = dict(algorithm=0, trees=5)
    search_params = dict()
    flann = cv2.FlannBasedMatcher(index_params, search_params)
    matches = flann.knnMatch(des1, des2, k=2)

    # Store all good matches as per Lowe's ratio test
    good_matches = []
    for m, n in matches:
        if m.distance < 0.7 * n.distance:
            good_matches.append(m)

    match_img = cv2.drawMatches(image1, kp1, image2, kp2, good_matches, None, flags=2)
    st.image(match_img, caption="SURF Feature Matches", channels="BGR")